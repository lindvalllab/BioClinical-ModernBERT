{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81acdafc",
   "metadata": {},
   "source": [
    "This notebook can be used to visualize the performance of the models that have been fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de13a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e308379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to the task outputs you would like to visualize\n",
    "results_path = \"../outputs/Phenotype\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "399c25f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract hyperparameters from the filename.\n",
    "    Expected format: lr=<lr>_wd=<wd>_epochs=<epochs>_seed=<seed>_effective_batch_size=<effective_bs>.txt\n",
    "    \"\"\"\n",
    "    pattern = r\"lr=([^_]+)_wd=([^_]+)_epochs=([^_]+)_seed=([^_]+)_effective_batch_size=([^\\.]+)\"\n",
    "    match = re.search(pattern, filename)\n",
    "    if match:\n",
    "        lr, wd, epochs, seed, effective_bs = match.groups()\n",
    "        return {\n",
    "            \"lr\": float(lr),\n",
    "            \"wd\": float(wd),\n",
    "            \"epochs\": int(epochs),\n",
    "            \"seed\": int(seed),\n",
    "            \"effective_batch_size\": int(effective_bs)\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def parse_log_file(filepath):\n",
    "    \"\"\"\n",
    "    Parse the log file content, which is assumed to contain lines of the form 'Key: Value'.\n",
    "    \n",
    "    This function ignores lines with keys in the ignore list so that the DataFrame doesn't pick up\n",
    "    headers or unwanted entries.\n",
    "    \"\"\"\n",
    "    ignore_keys = {\"test evaluation results\", \"epoch\", \"seed\", \"effective batch size\", \"eval_samples_per_second\", \"eval_steps_per_second\", \"eval_loss\"}\n",
    "    data = {}\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        # Skip lines that are empty or do not contain a colon.\n",
    "        if not line or \":\" not in line:\n",
    "            continue\n",
    "        key, value = line.split(\":\", 1)\n",
    "        key = key.strip()\n",
    "        # If the lowercase key is in the ignore list, skip it.\n",
    "        if key.lower() in ignore_keys:\n",
    "            continue\n",
    "        value = value.strip()\n",
    "        # If the value is an empty string, set it to np.nan.\n",
    "        if value == \"\":\n",
    "            data[key] = np.nan\n",
    "        else:\n",
    "            # Try to convert to a float if possible.\n",
    "            try:\n",
    "                # If value contains a decimal point then convert to float, otherwise try int.\n",
    "                if \".\" in value:\n",
    "                    data[key] = float(value)\n",
    "                else:\n",
    "                    data[key] = int(value)\n",
    "            except ValueError:\n",
    "                data[key] = value  # keep as string if conversion fails\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9590708",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "# Recursively find all .txt files in the directory.\n",
    "filepaths = glob.glob(os.path.join(results_path, \"**\", \"*.txt\"), recursive=True)\n",
    "if not filepaths:\n",
    "    print(f\"No log files found in {results_path}\")\n",
    "\n",
    "for filepath in filepaths:\n",
    "    filename = os.path.basename(filepath)\n",
    "    # Assume that the model name is the immediate parent folder.\n",
    "    model = os.path.basename(os.path.dirname(filepath))\n",
    "    \n",
    "    params = parse_filename(filename)\n",
    "    if params is None:\n",
    "        print(f\"Filename {filename} does not match the expected pattern. Skipping file.\")\n",
    "        continue\n",
    "    \n",
    "    log_data = parse_log_file(filepath)\n",
    "    \n",
    "    # Combine the data from the filename, file content, and model name.\n",
    "    record = {\n",
    "        \"Model\": model,\n",
    "        **params,  # Contains keys: lr, wd, epochs, seed, effective_batch_size\n",
    "    }\n",
    "    record.update(log_data)\n",
    "    records.append(record)\n",
    "\n",
    "# Create a DataFrame from the records.\n",
    "df = pd.DataFrame(records)\n",
    "print(\"Individual results:\")\n",
    "print(df.head(), \"\\n\")\n",
    "\n",
    "# Group on Model, lr, wd, and effective_batch_size.\n",
    "group_cols = [\"Model\", \"lr\", \"wd\", \"effective_batch_size\", \"epochs\"]\n",
    "ignore_cols = set(group_cols + [\"seed\"])\n",
    "metric_cols = [col for col in df.columns if col not in ignore_cols]\n",
    "\n",
    "# Convert each metric column to numeric (coercing errors to NaN).\n",
    "for col in metric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# Define aggregation: for each metric, compute mean, median, min, and max.\n",
    "agg_funcs = {col: [\"mean\", \"median\", \"min\", \"max\"] for col in metric_cols}\n",
    "grouped = df.groupby(group_cols).agg(agg_funcs)\n",
    "\n",
    "# Flatten the multi-level column index.\n",
    "grouped.columns = [\"_\".join(col).strip() for col in grouped.columns.values]\n",
    "grouped = grouped.reset_index()\n",
    "\n",
    "for col in grouped.columns:\n",
    "    if \"f1\" in col.lower():\n",
    "        grouped[col] = (grouped[col] * 100).round(1)\n",
    "\n",
    "\n",
    "print(\"Aggregated results over seeds:\")\n",
    "print(grouped.head())\n",
    "\n",
    "# Optionally, save the aggregated DataFrame to CSV.\n",
    "output_csv = os.path.join(results_path, \"aggregated_results.csv\")\n",
    "grouped.to_csv(output_csv, index=False)\n",
    "print(f\"\\nAggregated results saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc10033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.sort_values(by='accuracy_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d7383c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
